import typing as t

__all__ = ['ObjectDocumentModel', 'Paragraph', 'Sentence']

class _WordTokenizer(t.Protocol):
    def to_words(self, __text: str) -> t.Sequence[str]: ...

class ObjectDocumentModel(object):
    def __init__(self, paragraphs: t.Iterable[Paragraph]) -> None: ...
    @property
    def paragraphs(self) -> tuple[Paragraph, ...]: ...
    @property
    def sentences(self) -> tuple[Sentence, ...]: ...
    @property
    def headings(self) -> tuple[Sentence, ...]: ...
    @property
    def words(self) -> tuple[str, ...]: ...
    def __str__(self) -> str: ...

class Paragraph(object):
    def __init__(self, sentences: t.Iterable[Sentence]) -> None: ...
    @property
    def sentences(self) -> tuple[Sentence, ...]: ...
    @property
    def headings(self) -> tuple[Sentence, ...]: ...
    @property
    def words(self) -> tuple[str, ...]: ...
    def __str__(self) -> str: ...

class Sentence(object):
    def __init__(
        self, text: str | bytes, tokenizer: _WordTokenizer, is_heading: bool = False
    ): ...
    @property
    def words(self) -> t.Sequence[str]: ...
    @property
    def is_heading(self) -> bool: ...
    def __eq__(self, sentence: Sentence) -> bool: ...
    def __ne__(self, sentence: Sentence) -> bool: ...
    def __str__(self) -> str: ...

del t
