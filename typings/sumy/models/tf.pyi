import typing as t

__all__ = ['TfDocumentModel']

class _WordTokenizer(t.Protocol):
    def to_words(self, __text: str) -> t.Sequence[str]: ...

class TfDocumentModel(object):
    @t.overload
    def __init__(self, words: t.Sequence[str], tokenizer: t.Any = None) -> None: ...
    @t.overload
    def __init__(self, words: str | bytes, tokenizer: _WordTokenizer) -> None: ...
    @property
    def magnitude(self) -> float: ...
    @property
    def terms(self) -> t.KeysView[str]: ...
    def most_frequent_terms(self, count: int = 0) -> tuple[str, ...]: ...
    def term_frequency(self, term: str) -> int: ...
    def normalized_term_frequency(self, term: str, smooth: float = 0.0) -> float: ...

del t
