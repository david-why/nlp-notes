{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build TF-IDF From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import typing as t\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.sparse import csr_matrix, spmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    word_tokenize('hello world')\n",
    "except:\n",
    "    nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = requests.get('https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv')\n",
    "df = pd.read_csv(io.BytesIO(request.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ad sales boost Time Warner profit\\n\\nQuarterly...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dollar gains on Greenspan speech\\n\\nThe dollar...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yukos unit buyer faces loan claim\\n\\nThe owner...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High fuel prices hit BA's profits\\n\\nBritish A...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pernod takeover talk lifts Domecq\\n\\nShares in...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    labels\n",
       "0  Ad sales boost Time Warner profit\\n\\nQuarterly...  business\n",
       "1  Dollar gains on Greenspan speech\\n\\nThe dollar...  business\n",
       "2  Yukos unit buyer faces loan claim\\n\\nThe owner...  business\n",
       "3  High fuel prices hit BA's profits\\n\\nBritish A...  business\n",
       "4  Pernod takeover talk lifts Domecq\\n\\nShares in...  business"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(\n",
    "    *arrays: t.Union[pd.Series, np.ndarray, spmatrix],\n",
    "    seed: int = 123,\n",
    "    train_pct: float = 0.75\n",
    ") -> list:\n",
    "    def _getitems(array, indices):\n",
    "        if hasattr(array, 'iloc') or hasattr(array, 'shape'):\n",
    "            return array[indices]\n",
    "        return [array[i] for i in indices]\n",
    "    ret = []\n",
    "    for array in arrays:\n",
    "        rand = np.random.default_rng(seed)\n",
    "        perm = rand.permutation(len(array))\n",
    "        train_count = round(train_pct * len(array))\n",
    "        train_idx = perm[:train_count]\n",
    "        test_idx = perm[train_count:]\n",
    "        ret += [_getitems(array, train_idx), _getitems(array, test_idx)]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1669, 556)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = df['text']\n",
    "labels = df['labels']\n",
    "inputs_train, inputs_test, Ytrain, Ytest = train_test_split(inputs, labels, seed=12345)\n",
    "len(inputs_train), len(inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountVectorizer:\n",
    "    def __init__(self, ignore_case: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the count vectorizer.\n",
    "        :param ignore_case: Lowercase all documents before processing.\n",
    "        \"\"\"\n",
    "        self.mapping: t.Dict[str, int] = {}\n",
    "        self.reverse: t.Dict[int, str] = {}\n",
    "        self.ignore_case = ignore_case\n",
    "\n",
    "    def fit(self, documents: t.Iterable[str]) -> 'CountVectorizer':\n",
    "        \"\"\"\n",
    "        Fit the vectorizer to the given documents.\n",
    "        :param documents: The raw documents to fit against.\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.mapping.clear()\n",
    "        self.reverse.clear()\n",
    "        for document in documents:\n",
    "            if self.ignore_case:\n",
    "                document = document.lower()\n",
    "            tokens = word_tokenize(document)\n",
    "            for token in tokens:\n",
    "                if token not in self.mapping:\n",
    "                    self.reverse[len(self.mapping)] = token\n",
    "                    self.mapping[token] = len(self.mapping)\n",
    "        return self\n",
    "\n",
    "    def transform(self, documents: t.Iterable[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Vectorize the given documents.\n",
    "        :param documents: The raw documents to convert into vectors.\n",
    "        :return: A NumPy matrix of shape (len(documents), len(self.mapping)).\n",
    "        \"\"\"\n",
    "        mat = np.zeros((len(documents), len(self.mapping)), np.int32)\n",
    "        for i, document in enumerate(documents):\n",
    "            if self.ignore_case:\n",
    "                document = document.lower()\n",
    "            tokens = word_tokenize(document)\n",
    "            for token in tokens:\n",
    "                index = self.mapping.get(token)\n",
    "                if index is None:\n",
    "                    continue\n",
    "                mat[i, index] += 1\n",
    "        return mat\n",
    "\n",
    "    def fit_transform(self, documents: t.Iterable[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit against and vectorize the given documents. Eqivalent to calling\n",
    "        fit() and then transform().\n",
    "        :param documents: The raw documents to use.\n",
    "        :return: A NumPy matrix of shape (len(documents), len(self.mapping)).\n",
    "        \"\"\"\n",
    "        return self.fit(documents).transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "count_arr = vectorizer.fit_transform(inputs)\n",
    "count_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('entertainment',\n",
       " \"Police praise 'courageous' Ozzy\",\n",
       " [(31, '.', 35),\n",
       "  (23, 'the', 26),\n",
       "  (63, 'a', 23),\n",
       "  (27, ',', 20),\n",
       "  (46, 'and', 18)])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(54321)\n",
    "idx = np.random.choice(len(inputs))\n",
    "text = df.iloc[idx]\n",
    "(\n",
    "    text['labels'],\n",
    "    text['text'].partition('\\n')[0],\n",
    "    [(x, vectorizer.reverse[x], count_arr[idx][x]) for x in (-count_arr[idx]).argsort()[:5]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFWrapper(CountVectorizer):\n",
    "    def _transform(self, tf, documents):\n",
    "        # print(np.sum(tf), len(documents))\n",
    "        df = np.sum(tf > 0, axis=0)\n",
    "        idf = np.log(len(documents) / df)\n",
    "        tfidf = tf * idf\n",
    "        # print(np.sum(df==0), df.shape)\n",
    "        # print(tf.j, df, idf)\n",
    "        return tfidf - tfidf.min()\n",
    "\n",
    "    def transform(self, documents: t.Iterable[str]) -> np.ndarray:\n",
    "        return self._transform(super().transform(documents), documents)\n",
    "\n",
    "    def fit_transform(self, documents: t.Iterable[str]) -> np.ndarray:\n",
    "        self.fit(documents)\n",
    "        return self.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.22260554, 9.5575688 , 2.86332511, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 2.86332511, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 7.70751219, 7.70751219,\n",
       "        7.70751219]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TFIDFWrapper()\n",
    "tfidf_arr = tfidf.fit_transform(inputs)\n",
    "tfidf_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('entertainment',\n",
       " \"Police praise 'courageous' Ozzy\",\n",
       " [(16949, 'ozzy', 46.26229934152561),\n",
       "  (15888, 'osbourne', 22.512282611682018),\n",
       "  (12214, 'sharon', 21.61970840642518),\n",
       "  (9493, 'police', 21.24278352096904),\n",
       "  (16951, 'burglar', 19.826699717796693)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(54321)\n",
    "idx = np.random.choice(len(inputs))\n",
    "text = df.iloc[idx]\n",
    "(\n",
    "    text['labels'],\n",
    "    text['text'].partition('\\n')[0],\n",
    "    [(x, tfidf.reverse[x], tfidf_arr[idx][x]) for x in (-tfidf_arr[idx]).argsort()[:5]],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
