{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Vector Models and Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/david/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/david/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/david/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/david/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/david/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some key imports\n",
    "import nltk\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "# Download used nltk stuffs\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document', 'know', 'second']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second document.',\n",
    "    'What if this is the second document?',\n",
    "    'I know I am not the first OR second document.',\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase=True, analyzer='word', stop_words='english')\n",
    "# from nltk.corpus import stopwords\n",
    "# vectorizer = CountVectorizer(lowercase=True, analyzer='word', stop_words=stopwords.words('english'))\n",
    "vectorizer.fit(corpus)\n",
    "display(sorted(vectorizer.vocabulary_, key=vectorizer.vocabulary_.__getitem__))\n",
    "display(vectorizer.transform(corpus).toarray())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "assert (\n",
    "    stemmer.stem('walking') == stemmer.stem('walks') == stemmer.stem('walked')\n",
    "), 'the stemmer is buggy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "assert lemmatizer.lemmatize('mice') == 'mouse', 'the lemmatizer is buggy'\n",
    "assert lemmatizer.lemmatize('going') == 'going', 'no part of speech specified'\n",
    "assert lemmatizer.lemmatize('going', pos='v') == 'go', 'did not use verb rules'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'be',\n",
       " 'refuse',\n",
       " 'to',\n",
       " 'permit',\n",
       " 'u',\n",
       " 'to',\n",
       " 'obtain',\n",
       " 'the',\n",
       " 'refuse',\n",
       " 'permit']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "pos_mapping = {'ADJ': 'a', 'ADV': 'r', 'NOUN': 'n', 'VERB': 'v'}\n",
    "# from https://www.nltk.org/book/ch05.html\n",
    "sentence = 'They are refusing to permit us to obtain the refuse permit'\n",
    "tagged_tokens = pos_tag(word_tokenize(sentence), tagset='universal')\n",
    "lemmatized = []\n",
    "for token, pos in tagged_tokens:\n",
    "    lemmatized.append(lemmatizer.lemmatize(token, pos=pos_mapping.get(pos, 'n')))\n",
    "lemmatized\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer\n",
    "See [count-vectorizer.ipynb](count-vectorizer.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Exercise\n",
    "See [recommender.ipynb](recommender.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Word Embeddings\n",
    "See [word-embeddings.ipynb](word-embeddings.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
